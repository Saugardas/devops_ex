# 2. Выполнение предсказания модели в FastAPI-приложении

Дата: 2025-11-29

## Статус
Принято

## Контекст
Для сервиса классификации Ирисов Фишера необходимо реализовать API, возвращающее предсказания модели. Возникает вопрос: где выполнять предсказания — в том же процессе, что и FastAPI, или в отдельном микросервисе.

## Альтернативы

### 1. Встроенный инференс (in-process)
Модель загружается при старте FastAPI и используется напрямую в обработчике запроса `/predict`.

**Плюсы:**
- Минимальная задержка (нет сетевого вызова).
- Простое развертывание (один контейнер).
- Легко отлаживать.

**Минусы:**
- Связывание веб-логики и ML-логики в одном процессе.
- При потребностях масштабирования могут возникнуть трудности.

### 2. Отдельный ML-сервис
FastAPI-сервис вызывает другой сервис по HTTP/gRPC для получения предсказания.

**Плюсы:**
- Чёткое разделение ответственности.
- Возможность независимого масштабирования и обновления модели.

**Минусы:**
- Увеличение задержки из-за сетевого вызова.
- Сложность развертывания (нужно управлять двумя сервисами).
- Требуется обработка ошибок сети, таймаутов и retries.

## Решение
Выбран вариант 1 — встроенный инференс.

Проект не будет развиваться дальше, следовательно простота разворачивания даёт нам преимущества, в то время как минусы масштабирования отпадают на данном этапе.

## Последствия
- Архитектура остаётся монолитной, с разделением по модулям (API, ML, monitoring).
- При росте нагрузки или смене модели в будущем может потребоваться рефакторинг.
